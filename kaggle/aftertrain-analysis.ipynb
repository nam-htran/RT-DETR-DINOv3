{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab577a7",
   "metadata": {
    "_cell_guid": "1af65207-6577-4b81-b9f7-2aceeabd4b7f",
    "_uuid": "c1e1342d-d0c9-4bc7-9647-edae051d0346",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-31T07:24:15.338556Z",
     "iopub.status.busy": "2025-10-31T07:24:15.338354Z",
     "iopub.status.idle": "2025-10-31T07:24:15.346840Z",
     "shell.execute_reply": "2025-10-31T07:24:15.346040Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.013262,
     "end_time": "2025-10-31T07:24:15.348045",
     "exception": false,
     "start_time": "2025-10-31T07:24:15.334783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing upload_multiple_runs_to_separate_projects.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile upload_multiple_runs_to_separate_projects.py\n",
    "import wandb\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "BASE_PATH = '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL'\n",
    "\n",
    "runs_to_upload = [\n",
    "    {\n",
    "        'project_name': 'rtdetr-taco-convnext-teacher',\n",
    "        'output_dir': os.path.join(BASE_PATH, 'FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher'),\n",
    "        'config_file_path': os.path.join(BASE_PATH, 'CONFIG/rtdetrv2_taco_finetune_convnext.yml'),\n",
    "        'run_name': 'finetune_convnext_teacher',\n",
    "        'artifact_name': 'rtdetrv2-convnext-teacher-best'\n",
    "    },\n",
    "    {\n",
    "        'project_name': 'rtdetr-taco-vit-teacher',\n",
    "        'output_dir': os.path.join(BASE_PATH, 'FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher'),\n",
    "        'config_file_path': os.path.join(BASE_PATH, 'CONFIG/rtdetrv2_taco_finetune_vit.yml'),\n",
    "        'run_name': 'finetune_vit_teacher',\n",
    "        'artifact_name': 'rtdetrv2-vit-teacher-best'\n",
    "    },\n",
    "    {\n",
    "        'project_name': 'rtdetr-taco-baseline',\n",
    "        'output_dir': os.path.join(BASE_PATH, 'FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE'),\n",
    "        'config_file_path': os.path.join(BASE_PATH, 'CONFIG/rtdetrv2_taco_finetune_BASELINE.yml'),\n",
    "        'run_name': 'finetune_baseline',\n",
    "        'artifact_name': 'rtdetrv2-baseline-best'\n",
    "    },\n",
    "]\n",
    "\n",
    "def upload_single_run(project_name, output_dir, config_file_path, run_name, artifact_name):\n",
    "    log_file_path = os.path.join(output_dir, 'log.txt')\n",
    "    checkpoint_file_path = os.path.join(output_dir, 'best.pth')\n",
    "\n",
    "    print(f\"\\n--- Starting upload for project: {project_name}, run: {run_name} ---\")\n",
    "\n",
    "    if not os.path.exists(log_file_path):\n",
    "        print(f\"ERROR: Log file not found, skipping run: {log_file_path}\")\n",
    "        return\n",
    "    if not os.path.exists(checkpoint_file_path):\n",
    "        print(f\"ERROR: Checkpoint file not found, skipping run: {checkpoint_file_path}\")\n",
    "        return\n",
    "\n",
    "    run_config = None\n",
    "    try:\n",
    "        with open(config_file_path, 'r') as f:\n",
    "            run_config = yaml.safe_load(f)\n",
    "        \n",
    "        if run_config and '__include__' in run_config:\n",
    "            del run_config['__include__']\n",
    "            \n",
    "        print(f\"Successfully read and processed config file: {config_file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Config file not found at '{config_file_path}'. Skipping config logging.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not read config file. Error: {e}\")\n",
    "        \n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        config=run_config,\n",
    "        job_type='manual-upload',\n",
    "        reinit=True\n",
    "    )\n",
    "\n",
    "    print(f\"Parsing and logging metrics from: {log_file_path}\")\n",
    "    with open(log_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                log_data = json.loads(line)\n",
    "                epoch = log_data.get('epoch')\n",
    "                if epoch is not None:\n",
    "                    metrics_to_log = {k: v for k, v in log_data.items() if k not in ['epoch', 'n_parameters']}\n",
    "                    wandb.log(metrics_to_log, step=epoch)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse line in log.txt: {line.strip()}\")\n",
    "\n",
    "    print(f\"Logging model artifact from: {checkpoint_file_path}\")\n",
    "    artifact = wandb.Artifact(name=artifact_name, type='model')\n",
    "    artifact.add_file(checkpoint_file_path)\n",
    "    run.log_artifact(artifact)\n",
    "\n",
    "    run.finish()\n",
    "    print(f\"--- Finished upload for run: {run_name} ---\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        secrets = UserSecretsClient()\n",
    "        wandb_key = secrets.get_secret(\"WANDB_API_KEY\")\n",
    "        wandb.login(key=wandb_key)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log in to W&B. Please ensure 'WANDB_API_KEY' is set in Kaggle Secrets. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    for run_info in runs_to_upload:\n",
    "        upload_single_run(\n",
    "            project_name=run_info['project_name'],\n",
    "            output_dir=run_info['output_dir'],\n",
    "            config_file_path=run_info['config_file_path'],\n",
    "            run_name=run_info['run_name'],\n",
    "            artifact_name=run_info['artifact_name']\n",
    "        )\n",
    "    \n",
    "    print(\"\\nAll specified runs have been uploaded.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63c9cc5",
   "metadata": {
    "_cell_guid": "6c1b5dac-b6d4-4e9a-bba6-ec062ec2d12d",
    "_uuid": "417f7896-aad1-4de2-b672-15ac6b8fb8c2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-31T07:24:15.352979Z",
     "iopub.status.busy": "2025-10-31T07:24:15.352626Z",
     "iopub.status.idle": "2025-10-31T07:24:47.164455Z",
     "shell.execute_reply": "2025-10-31T07:24:47.163509Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 31.815948,
     "end_time": "2025-10-31T07:24:47.166109",
     "exception": false,
     "start_time": "2025-10-31T07:24:15.350161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\r\n",
      "  warnings.warn(\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnamthse182380\u001b[0m (\u001b[33mnamthse182380-fpt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n",
      "\r\n",
      "--- Starting upload for project: rtdetr-taco-convnext-teacher, run: finetune_convnext_teacher ---\r\n",
      "Successfully read and processed config file: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/CONFIG/rtdetrv2_taco_finetune_convnext.yml\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251031_072418-p99so6pb\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfinetune_convnext_teacher\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-convnext-teacher\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-convnext-teacher/runs/p99so6pb\u001b[0m\r\n",
      "Parsing and logging metrics from: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/log.txt\r\n",
      "Logging model artifact from: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/best.pth\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▇▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_bbox █▇▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_0 █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_1 █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_2 █▇▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_3 █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_4 █▇▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_0 ▇▇█▇▇▆▆▅▆▅▅▄▅▄▄▄▄▃▃▃▃▂▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_1 ▇▇██▇▆▆▅▅▅▄▄▅▄▄▄▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_2 █▇█▇▇▆▆▅▅▅▅▄▅▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_3 ▇▇██▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_4 ▇▇██▇▆▆▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_5 ▇██▇▇▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_enc_0 █▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_giou █▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_0 █▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_1 █▆▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_2 █▇▆▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_3 █▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_4 █▇▆▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_0 █████▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_1 ████▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_2 ████▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_3 ████▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_4 ████▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_5 ████▇▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_enc_0 █▆▆▅▅▄▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss_vfl ▁▃▄▅▆▆▇▆▇▇▇▇██████████████████▇▇█▇▇▇▇▇▇▇\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_0 ▁▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██▇███████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_1 ▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇█████████████████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_2 ▁▃▃▄▅▆▆▆▆▇▇▇▇█████████████████████▇█████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_3 ▁▃▃▄▅▆▆▆▇▇▇▇▇███████████████████▇▇▇▇▇▇▇█\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_4 ▁▃▃▄▅▆▆▆▆▇▇▇██████████████████▇█▇▇▇▇▇▇▇▇\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_0 █▃▁▁▁▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_1 █▃▁▁▂▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_2 █▃▁▁▂▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_3 █▄▁▁▂▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_4 █▄▁▁▃▅▆▆▆▆▆▆▆▆▅▆▆▅▅▅▅▅▅▅▅▅▄▅▄▄▄▄▄▄▄▄▃▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_5 █▄▁▁▃▅▇▇▆▇▆▆▆▆▆▅▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_enc_0 ▁▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train_lr ████████████████████████████████▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss 26.61384\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_bbox 0.26072\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_0 0.29172\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_1 0.27414\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_2 0.26629\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_3 0.2648\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_4 0.26287\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_0 0.34207\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_1 0.2854\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_2 0.2678\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_3 0.26112\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_4 0.25825\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_5 0.2581\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_enc_0 0.34384\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_giou 0.97439\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_0 1.02135\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_1 1.00007\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_2 0.98671\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_3 0.98106\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_4 0.97707\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_0 1.00388\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_1 0.89365\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_2 0.85906\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_3 0.84643\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_4 0.84028\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_5 0.84016\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_enc_0 1.11113\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss_vfl 1.02226\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_0 1.14318\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_1 1.10797\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_2 1.08293\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_3 1.06136\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_4 1.04429\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_0 0.53915\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_1 0.52587\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_2 0.51761\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_3 0.50916\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_4 0.50457\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_5 0.49776\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_enc_0 1.08536\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train_lr 0.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfinetune_convnext_teacher\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-convnext-teacher/runs/p99so6pb\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-convnext-teacher\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251031_072418-p99so6pb/logs\u001b[0m\r\n",
      "--- Finished upload for run: finetune_convnext_teacher ---\r\n",
      "\r\n",
      "--- Starting upload for project: rtdetr-taco-vit-teacher, run: finetune_vit_teacher ---\r\n",
      "Successfully read and processed config file: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/CONFIG/rtdetrv2_taco_finetune_vit.yml\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251031_072429-uxp320mu\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfinetune_vit_teacher\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-vit-teacher\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-vit-teacher/runs/uxp320mu\u001b[0m\r\n",
      "Parsing and logging metrics from: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/log.txt\r\n",
      "Logging model artifact from: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▇▇▆▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_bbox █▇▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_0 █▇▇▇▆▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_1 █▇▇▆▆▄▄▄▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_2 █▇▇▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_3 █▇▇▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_4 █▇▇▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_0 ▆▇█▆▇▇▇▆▇▆▅▄▅▅▄▄▄▃▄▃▄▃▂▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▂▂\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_1 ▆▆▇█▇▆▇▇▆▆▅▅▄▅▄▅▄▄▃▄▄▄▃▂▄▂▃▃▂▂▂▂▂▂▁▂▁▂▁▂\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_2 ▇▇██▇▆▇▆▆▆▅▅▄▅▄▄▄▄▃▃▃▃▂▃▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▂\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_3 ▇██▇▇▇▆▆▆▅▄▅▅▄▅▄▄▄▃▄▃▂▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_4 ▇▇██▇▆▇▆▆▆▅▅▄▅▅▅▄▄▄▄▄▃▃▃▃▂▃▃▂▂▂▂▂▁▁▁▁▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_5 ▇▇██▇▇▇▇▆▆▅▄▅▅▄▄▄▄▄▃▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_enc_0 █▇▇▇▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_giou █▇▆▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_0 █▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_1 █▇▆▆▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_2 █▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_3 █▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_4 █▇▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_0 ███████▇▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_1 ████████▇▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_2 ███████▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_3 ██████▇▇▆▆▅▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_4 ██████▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_5 █████▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_enc_0 █▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss_vfl ▁▂▃▄▄▄▅▅▆▆▇▇▇▇▇▇█▇█▇████████████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_0 ▁▂▂▃▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇██████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_1 ▁▂▃▃▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_2 ▁▂▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇██▇███▇▇█████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_3 ▁▂▃▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇███████▇████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_4 ▁▂▃▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇████████████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_0 █▅▃▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_1 █▅▃▂▂▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▄▃▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_2 █▅▄▃▂▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_3 █▅▄▃▂▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_4 █▅▄▃▂▁▁▂▂▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_5 █▄▄▃▂▁▁▂▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_enc_0 ▁▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█▇█▇▇████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train_lr ████████████████████████████████▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss 29.40929\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_bbox 0.32245\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_0 0.35221\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_1 0.33993\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_2 0.33161\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_3 0.32977\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_4 0.32625\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_0 0.4187\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_1 0.38256\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_2 0.36574\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_3 0.35587\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_4 0.3499\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_5 0.34896\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_enc_0 0.38811\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_giou 1.0443\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_0 1.08422\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_1 1.06827\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_2 1.05585\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_3 1.05262\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_4 1.04573\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_0 1.12276\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_1 1.03425\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_2 0.99748\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_3 0.97719\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_4 0.96534\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_5 0.96385\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_enc_0 1.16915\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss_vfl 1.11524\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_0 1.14826\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_1 1.13623\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_2 1.12763\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_3 1.12535\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_4 1.12592\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_0 0.50377\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_1 0.54217\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_2 0.55847\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_3 0.579\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_4 0.59599\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_5 0.60416\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_enc_0 1.05402\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train_lr 0.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfinetune_vit_teacher\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-vit-teacher/runs/uxp320mu\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-vit-teacher\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251031_072429-uxp320mu/logs\u001b[0m\r\n",
      "--- Finished upload for run: finetune_vit_teacher ---\r\n",
      "\r\n",
      "--- Starting upload for project: rtdetr-taco-baseline, run: finetune_baseline ---\r\n",
      "Successfully read and processed config file: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/CONFIG/rtdetrv2_taco_finetune_BASELINE.yml\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251031_072437-pv37o4st\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfinetune_baseline\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-baseline\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-baseline/runs/pv37o4st\u001b[0m\r\n",
      "Parsing and logging metrics from: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/log.txt\r\n",
      "Logging model artifact from: /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/best.pth\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss █▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_bbox █▇▇▇▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_0 █▇▇▇▆▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_1 █▇▇▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_2 █▇▇▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_3 █▇▇▇▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_4 █▇▇▇▆▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_0 ▆▆▇█▇▆▇▇▆▇▅▅▄▅▅▄▄▃▄▃▃▃▂▄▃▃▃▂▂▂▂▂▂▂▁▁▂▁▂▂\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_1 ▆▇▇█▇▆▇▇▆▆▅▅▄▄▄▄▄▃▄▃▃▃▂▄▃▃▃▂▂▂▂▂▂▁▁▁▂▁▂▂\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_2 ▇▇██▇▆▇▆▆▆▅▅▄▅▅▄▄▃▄▃▃▃▂▃▃▃▃▂▂▁▂▂▂▁▁▁▂▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_3 ▇▇██▇▆▇▆▆▆▅▅▄▅▄▄▄▄▃▄▃▃▃▂▃▂▂▃▂▂▁▂▁▂▁▂▁▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_4 ▇▇██▇▆▇▆▆▆▅▅▄▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_5 ▇▇█▇█▇▇▆▆▅▅▅▅▄▅▄▄▄▄▄▃▃▄▃▂▃▂▂▂▂▂▂▂▁▁▁▂▁▂▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_enc_0 █▇▇▇▆▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_giou █▇▆▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_0 █▇▆▆▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_1 █▇▆▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_2 █▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_3 █▇▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_4 █▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_0 ███████▇▇▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_1 ███████▇▇▇▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_2 █████▇▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_3 ██████▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_4 ██████▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_5 ██████▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_enc_0 █▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss_vfl ▁▂▃▄▄▄▅▆▆▆▇▇▇▇▇▇█▇▇█▇███████████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_0 ▁▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇██████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_1 ▁▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇███▇█████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_2 ▁▂▃▃▄▅▅▅▆▆▇▇▇▇▇▇▇█▇█▇████▇██████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_3 ▁▂▃▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_4 ▁▂▃▄▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇███▇█████████████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_0 █▅▃▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_1 █▅▃▂▂▁▁▁▁▂▂▂▂▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_2 █▅▄▃▂▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_3 █▅▄▃▂▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_4 █▅▄▃▂▁▂▂▂▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_5 █▄▃▂▁▁▂▂▂▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_enc_0 ▁▂▂▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train_lr ████████████████████████████████▁▁▁▁▁▁▁▁\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train_loss 29.36723\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_bbox 0.3184\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_0 0.3508\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_1 0.3388\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_2 0.328\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_3 0.32467\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_aux_4 0.32057\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_0 0.41546\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_1 0.38074\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_2 0.36413\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_3 0.35437\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_4 0.34898\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_bbox_dn_5 0.3479\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_bbox_enc_0 0.39295\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train_loss_giou 1.03996\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_0 1.08211\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_1 1.06737\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_2 1.0532\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_3 1.04668\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_aux_4 1.04155\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_0 1.11975\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_1 1.03405\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_2 0.99578\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_3 0.97655\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_4 0.96426\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_giou_dn_5 0.9626\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss_giou_enc_0 1.16765\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        train_loss_vfl 1.12133\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_0 1.14268\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_1 1.12916\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_2 1.12784\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_3 1.13213\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_aux_4 1.13026\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_0 0.50819\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_1 0.54441\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_2 0.56253\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_3 0.58247\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_4 0.59907\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train_loss_vfl_dn_5 0.60776\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_loss_vfl_enc_0 1.04211\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train_lr 0.0\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfinetune_baseline\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-baseline/runs/pv37o4st\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/namthse182380-fpt-university/rtdetr-taco-baseline\u001b[0m\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251031_072437-pv37o4st/logs\u001b[0m\r\n",
      "--- Finished upload for run: finetune_baseline ---\r\n",
      "\r\n",
      "All specified runs have been uploaded.\r\n"
     ]
    }
   ],
   "source": [
    "!python upload_multiple_runs_to_separate_projects.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99aca4d",
   "metadata": {
    "_cell_guid": "1e3935d8-84c5-4dbc-88da-2b43eec718e8",
    "_uuid": "b47cc45d-3ab2-47b3-b9bb-3dc5b549a75e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-31T07:24:47.175384Z",
     "iopub.status.busy": "2025-10-31T07:24:47.175058Z",
     "iopub.status.idle": "2025-10-31T07:26:11.996474Z",
     "shell.execute_reply": "2025-10-31T07:26:11.995463Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 84.828229,
     "end_time": "2025-10-31T07:26:11.998038",
     "exception": false,
     "start_time": "2025-10-31T07:24:47.169809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m579.2/579.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\r\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\r\n",
      "mkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "mkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "mkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\r\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "ydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\r\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\r\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\r\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q thop ultralytics pycocotools faster-coco-eval protobuf==3.20.3\n",
    "!pip install -q \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6569b9b5",
   "metadata": {
    "_cell_guid": "39290208-5f0a-452c-838c-10da4e95532f",
    "_uuid": "bbc7210b-1b1b-426f-9570-fe43f912dd68",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-31T07:26:12.044133Z",
     "iopub.status.busy": "2025-10-31T07:26:12.043890Z",
     "iopub.status.idle": "2025-10-31T07:26:12.055371Z",
     "shell.execute_reply": "2025-10-31T07:26:12.054572Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.035796,
     "end_time": "2025-10-31T07:26:12.056505",
     "exception": false,
     "start_time": "2025-10-31T07:26:12.020709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/final_benchmark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/final_benchmark.py\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import warnings\n",
    "import yaml\n",
    "import shutil\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "RTDETR_REPO_PATH = \"/kaggle/working/RT-DETR\"\n",
    "if not os.path.exists(RTDETR_REPO_PATH):\n",
    "    print(f\"ERROR: Repository not found at '{RTDETR_REPO_PATH}'. Please run the git clone cell first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "rtdetr_src_path = os.path.join(RTDETR_REPO_PATH, \"rtdetrv2_pytorch\")\n",
    "if rtdetr_src_path not in sys.path:\n",
    "    sys.path.insert(0, rtdetr_src_path)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from thop import profile\n",
    "from src.core import YAMLConfig\n",
    "\n",
    "def run_command(command, working_dir=None):\n",
    "    print(f\"\\n---> Executing command: {' '.join(command)}\")\n",
    "    full_output = []\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "            text=True, encoding='utf-8', cwd=working_dir\n",
    "        )\n",
    "        for line in process.stdout:\n",
    "            print(line.strip())\n",
    "            full_output.append(line.strip())\n",
    "        process.wait()\n",
    "        print(f\"--- Command {'succeeded' if process.returncode == 0 else f'failed with exit code {process.returncode}'} ---\")\n",
    "        return process.returncode, \"\\n\".join(full_output)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while executing command: {e}\")\n",
    "        return -1, str(e)\n",
    "\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\"*80 + f\"\\n| {title:^76} |\\n\" + \"=\"*80)\n",
    "\n",
    "def merge_yamls(base, new):\n",
    "    for key, value in new.items():\n",
    "        if key in base and isinstance(base[key], dict) and isinstance(value, dict): merge_yamls(base[key], value)\n",
    "        else: base[key] = value\n",
    "    return base\n",
    "\n",
    "def flatten_yaml_config(config_path, repo_configs_dir):\n",
    "    with open(config_path, 'r') as f: config = yaml.safe_load(f) or {}\n",
    "    if '__include__' in config:\n",
    "        includes = config.pop('__include__')\n",
    "        base_config = {}\n",
    "        for include_path_relative in includes:\n",
    "            include_filename = os.path.basename(include_path_relative)\n",
    "            found_path = next((os.path.join(root, include_filename) for root, _, files in os.walk(repo_configs_dir) if include_filename in files), None)\n",
    "            if found_path:\n",
    "                included_config = flatten_yaml_config(found_path, repo_configs_dir)\n",
    "                base_config = merge_yamls(base_config, included_config)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Could not find included file '{include_filename}' within '{repo_configs_dir}'\")\n",
    "        config = merge_yamls(base_config, config)\n",
    "    return config\n",
    "\n",
    "def parse_rtdetr_output(output_text):\n",
    "    metrics = {}\n",
    "    try:\n",
    "        ap_50_95_line = re.search(r\"Average Precision\\s+\\(AP\\) @\\[ IoU=0.50:0.95 .* = ([\\d\\.]+)\", output_text)\n",
    "        ap_50_line = re.search(r\"Average Precision\\s+\\(AP\\) @\\[ IoU=0.50\\s+.* = ([\\d\\.]+)\", output_text)\n",
    "        if ap_50_95_line: metrics['mAP50-95'] = float(ap_50_95_line.group(1))\n",
    "        if ap_50_line: metrics['mAP50'] = float(ap_50_line.group(1))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse RT-DETR metrics: {e}\")\n",
    "    return metrics\n",
    "\n",
    "def prepare_yolo_dataset():\n",
    "    print_header(\"Preparing Dataset for YOLO\")\n",
    "    YOLO_DATA_ROOT = '/kaggle/working/taco_yolo'\n",
    "    IMAGE_TRAIN_DIR_SRC = '/kaggle/input/dsp-pre-final/processed_taco_coco/train2017'\n",
    "    IMAGE_VAL_DIR_SRC = '/kaggle/input/dsp-pre-final/processed_taco_coco/val2017'\n",
    "    COCO_ANNOTATIONS_TRAIN = '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_train2017.json'\n",
    "    IMAGE_TRAIN_DIR_DEST = os.path.join(YOLO_DATA_ROOT, 'images', 'train')\n",
    "    IMAGE_VAL_DIR_DEST = os.path.join(YOLO_DATA_ROOT, 'images', 'val')\n",
    "    LABEL_TRAIN_DIR_DEST = os.path.join(YOLO_DATA_ROOT, 'labels', 'train')\n",
    "    LABEL_VAL_DIR_DEST = os.path.join(YOLO_DATA_ROOT, 'labels', 'val')\n",
    "    for d in [IMAGE_TRAIN_DIR_DEST, IMAGE_VAL_DIR_DEST, LABEL_TRAIN_DIR_DEST, LABEL_VAL_DIR_DEST]: os.makedirs(d, exist_ok=True)\n",
    "    os.system(f'cp -r {IMAGE_TRAIN_DIR_SRC}/* {IMAGE_TRAIN_DIR_DEST}/ 2>/dev/null')\n",
    "    os.system(f'cp -r {IMAGE_VAL_DIR_SRC}/* {IMAGE_VAL_DIR_DEST}/ 2>/dev/null')\n",
    "    def convert_coco_to_yolo(json_file, output_labels_dir):\n",
    "        with open(json_file) as f: data = json.load(f)\n",
    "        images_map = {img['id']: (img['file_name'], img['width'], img['height']) for img in data['images']}\n",
    "        for ann in tqdm(data['annotations'], desc=f\"Converting {os.path.basename(json_file)}\"):\n",
    "            image_id, class_id = ann['image_id'], ann['category_id']\n",
    "            if image_id not in images_map: continue\n",
    "            file_name, img_w, img_h = images_map[image_id]\n",
    "            box = ann['bbox']\n",
    "            x, y, w, h = box; x_center, y_center = (x + w / 2) / img_w, (y + h / 2) / img_h; norm_w, norm_h = w / img_w, h / img_h\n",
    "            label_file_name = os.path.splitext(file_name)[0] + '.txt'\n",
    "            with open(os.path.join(output_labels_dir, label_file_name), 'a') as f: f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {norm_w:.6f} {norm_h:.6f}\\n\")\n",
    "    convert_coco_to_yolo(COCO_ANNOTATIONS_TRAIN, LABEL_TRAIN_DIR_DEST)\n",
    "    convert_coco_to_yolo('/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_val2017.json', LABEL_VAL_DIR_DEST)\n",
    "    with open(COCO_ANNOTATIONS_TRAIN) as f: coco_data = json.load(f)\n",
    "    categories = sorted(coco_data['categories'], key=lambda x: x['id']); class_names = [cat['name'] for cat in categories]\n",
    "    taco_yaml_content = {'path': YOLO_DATA_ROOT, 'train': 'images/train', 'val': 'images/val', 'nc': len(class_names), 'names': class_names}\n",
    "    YAML_PATH = os.path.join(YOLO_DATA_ROOT, 'taco.yaml')\n",
    "    with open(YAML_PATH, 'w') as f: yaml.dump(taco_yaml_content, f, sort_keys=False)\n",
    "    print(f\"YOLO dataset YAML created at: {YAML_PATH}\")\n",
    "    return YAML_PATH\n",
    "\n",
    "def setup_and_verify(base_path):\n",
    "    print_header(\"Step 1: Verifying Checkpoint and Config Files\")\n",
    "    paths = {\n",
    "        \"Distill ConvNext Finetune\": { \"type\": \"rtdetr\", \"weights\": f\"{base_path}/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/best.pth\", \"config_src\": f\"{base_path}/CONFIG/rtdetrv2_taco_finetune_convnext.yml\" },\n",
    "        \"Distill ViT Finetune\": { \"type\": \"rtdetr\", \"weights\": f\"{base_path}/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth\", \"config_src\": f\"{base_path}/CONFIG/rtdetrv2_taco_finetune_vit.yml\" },\n",
    "        \"RT-DETR Baseline Finetune\": { \"type\": \"rtdetr\", \"weights\": f\"{base_path}/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/best.pth\", \"config_src\": f\"{base_path}/CONFIG/rtdetrv2_taco_finetune_BASELINE.yml\" },\n",
    "        \"YOLOv11 Finetune\": { \"type\": \"yolo\", \"weights\": f\"{base_path}/YOLO/yolo_checkpoints/yolo11l_finetune_baseline/weights/best.pt\"}\n",
    "    }\n",
    "    found_paths = {}\n",
    "    for name, path_dict in paths.items():\n",
    "        if all(os.path.exists(p) for k, p in path_dict.items() if k != 'type'):\n",
    "            print(f\"[FOUND] {name}\"); found_paths[name] = path_dict\n",
    "        else: print(f\"[SKIPPING] {name} - Missing files.\")\n",
    "    if not found_paths: sys.exit(1)\n",
    "    print(\"\\n---> All required files are ready!\")\n",
    "    return found_paths\n",
    "\n",
    "def evaluate_models(paths, yolo_data_yaml, YOLO_class, benchmark_results):\n",
    "    print_header(\"Step 2: Evaluating Performance (APval / AP50val)\")\n",
    "    rtdetr_work_dir = \"/kaggle/working/RT-DETR/rtdetrv2_pytorch\"\n",
    "    repo_configs_dir = os.path.join(rtdetr_work_dir, \"configs\")\n",
    "    for name, path_dict in paths.items():\n",
    "        benchmark_results[name] = {}\n",
    "        print(f\"\\n--- Evaluating: {name} ---\")\n",
    "        if path_dict[\"type\"] == \"rtdetr\":\n",
    "            config_data = flatten_yaml_config(path_dict[\"config_src\"], repo_configs_dir)\n",
    "            if 'tuning' in config_data: del config_data['tuning']\n",
    "            if 'val_dataloader' in config_data:\n",
    "                config_data['val_dataloader']['dataset']['img_folder'] = \"/kaggle/input/dsp-pre-final/processed_taco_coco/val2017\"\n",
    "                config_data['val_dataloader']['dataset']['ann_file'] = \"/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_val2017.json\"\n",
    "            temp_config_path = \"/kaggle/working/temp_flattened_config.yml\"\n",
    "            with open(temp_config_path, 'w') as f: yaml.dump(config_data, f)\n",
    "            command = [\"python\", \"tools/train.py\", \"-c\", temp_config_path, \"-r\", path_dict[\"weights\"], \"--test-only\"]\n",
    "            return_code, output = run_command(command, rtdetr_work_dir)\n",
    "            if return_code == 0: benchmark_results[name].update(parse_rtdetr_output(output))\n",
    "            if os.path.exists(temp_config_path): os.remove(temp_config_path)\n",
    "        elif path_dict[\"type\"] == \"yolo\":\n",
    "            model = YOLO_class(path_dict[\"weights\"])\n",
    "            results = model.val(data=yolo_data_yaml, imgsz=640, batch=16, split='val')\n",
    "            if results:\n",
    "                benchmark_results[name]['mAP50-95'] = results.box.map\n",
    "                benchmark_results[name]['mAP50'] = results.box.map50\n",
    "    \n",
    "def calculate_complexity(paths, YOLO_class, profile_func, YAMLConfig_class, benchmark_results):\n",
    "    print_header(\"Step 3: Analyzing Model Complexity (Params & FLOPs)\")\n",
    "    dummy_input = torch.randn(1, 3, 640, 640)\n",
    "    rtdetr_work_dir = \"/kaggle/working/RT-DETR/rtdetrv2_pytorch\"\n",
    "    repo_configs_dir = os.path.join(rtdetr_work_dir, \"configs\")\n",
    "    for name, path_dict in paths.items():\n",
    "        print(f\"\\n--- Analyzing: {name} ---\")\n",
    "        try:\n",
    "            model_cpu = None\n",
    "            if path_dict[\"type\"] == \"rtdetr\":\n",
    "                config_data = flatten_yaml_config(path_dict[\"config_src\"], repo_configs_dir)\n",
    "                temp_config_path = f\"/kaggle/working/temp_complexity_{name.replace(' ', '_')}.yml\"\n",
    "                with open(temp_config_path, 'w') as f: yaml.dump(config_data, f)\n",
    "                cfg = YAMLConfig(temp_config_path)\n",
    "                model_cpu = cfg.model.cpu()\n",
    "                if os.path.exists(temp_config_path): os.remove(temp_config_path)\n",
    "            elif path_dict[\"type\"] == \"yolo\":\n",
    "                model_cpu = YOLO_class(path_dict[\"weights\"]).model.cpu()\n",
    "            if model_cpu:\n",
    "                macs, params = profile_func(model_cpu, inputs=(dummy_input.cpu(),), verbose=False)\n",
    "                benchmark_results[name]['Params (M)'] = params / 1e6\n",
    "                benchmark_results[name]['FLOPs (G)'] = macs * 2 / 1e9\n",
    "                print(f\"Params: {benchmark_results[name]['Params (M)']:.2f} M, FLOPs: {benchmark_results[name]['FLOPs (G)']:.2f} G\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name}: {e}\")\n",
    "\n",
    "def measure_inference_speed(paths, YOLO_class, YAMLConfig_class, benchmark_results):\n",
    "    print_header(\"Step 4: Measuring Inference Speed\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dummy_input = torch.randn(1, 3, 640, 640, device=device)\n",
    "    warmup_runs = 20\n",
    "    timed_runs = 50\n",
    "    rtdetr_work_dir = \"/kaggle/working/RT-DETR/rtdetrv2_pytorch\"\n",
    "    repo_configs_dir = os.path.join(rtdetr_work_dir, \"configs\")\n",
    "\n",
    "    for name, path_dict in paths.items():\n",
    "        print(f\"\\n--- Measuring: {name} ---\")\n",
    "        try:\n",
    "            model = None\n",
    "            if path_dict[\"type\"] == \"rtdetr\":\n",
    "                config_data = flatten_yaml_config(path_dict[\"config_src\"], repo_configs_dir)\n",
    "                temp_config_path = f\"/kaggle/working/temp_speed_{name.replace(' ', '_')}.yml\"\n",
    "                with open(temp_config_path, 'w') as f: yaml.dump(config_data, f)\n",
    "                cfg = YAMLConfig(temp_config_path)\n",
    "                if os.path.exists(temp_config_path): os.remove(temp_config_path)\n",
    "                \n",
    "                model = cfg.model\n",
    "                state_dict = torch.load(path_dict[\"weights\"], map_location=device)\n",
    "                if 'model' in state_dict:\n",
    "                    model.load_state_dict(state_dict['model'])\n",
    "                else:\n",
    "                    model.load_state_dict(state_dict)\n",
    "                model.to(device).eval()\n",
    "\n",
    "            elif path_dict[\"type\"] == \"yolo\":\n",
    "                model = YOLO_class(path_dict[\"weights\"]).model.to(device).eval()\n",
    "\n",
    "            if model:\n",
    "                with torch.no_grad():\n",
    "                    for _ in range(warmup_runs): _ = model(dummy_input)\n",
    "                    torch.cuda.synchronize()\n",
    "                    start_time = time.time()\n",
    "                    for _ in range(timed_runs): _ = model(dummy_input)\n",
    "                    torch.cuda.synchronize()\n",
    "                    end_time = time.time()\n",
    "                avg_time_ms = (end_time - start_time) / timed_runs * 1000\n",
    "                benchmark_results[name]['Speed (ms)'] = avg_time_ms\n",
    "                print(f\"Average Inference Time: {avg_time_ms:.2f} ms/image\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error measuring speed for {name}: {e}\")\n",
    "\n",
    "def generate_summary(benchmark_results):\n",
    "    print_header(\"Final Benchmark Summary\")\n",
    "    df = pd.DataFrame.from_dict(benchmark_results, orient='index')\n",
    "    column_order = ['mAP50-95', 'mAP50', 'Speed (ms)', 'Params (M)', 'FLOPs (G)']\n",
    "    df = df[[col for col in column_order if col in df.columns]]\n",
    "    for col in ['mAP50-95', 'mAP50']:\n",
    "        if col in df.columns: df[col] = df[col].map('{:.4f}'.format)\n",
    "    for col in ['Params (M)', 'FLOPs (G)', 'Speed (ms)']:\n",
    "        if col in df.columns: df[col] = df[col].map('{:.2f}'.format)\n",
    "    print(f\"Benchmark conducted on: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "    print(df.to_string())\n",
    "    df.to_csv(\"benchmark_summary.csv\")\n",
    "    print(\"\\nSummary table saved to benchmark_summary.csv\")\n",
    "    try:\n",
    "        df_plot = df.astype(float)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df_plot['Speed (ms)'], df_plot['mAP50-95'])\n",
    "        for i, txt in enumerate(df_plot.index):\n",
    "            plt.annotate(txt, (df_plot['Speed (ms)'].iloc[i], df_plot['mAP50-95'].iloc[i]), ha='right', va='bottom')\n",
    "        plt.title('Benchmark: Speed vs. Accuracy')\n",
    "        plt.xlabel('Inference Time (ms)')\n",
    "        plt.ylabel('mAP @ .50-.95')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"benchmark_plot.png\")\n",
    "        plt.show()\n",
    "        print(\"Summary plot saved to benchmark_plot.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate plot: {e}\")\n",
    "\n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    benchmark_results = OrderedDict()\n",
    "    yolo_data_yaml_path = prepare_yolo_dataset()\n",
    "    base_data_path = \"/kaggle/input/rt-detr-dinov3-distilled-model/FINAL\"\n",
    "    paths = setup_and_verify(base_data_path)\n",
    "    if paths:\n",
    "        evaluate_models(paths, yolo_data_yaml_path, YOLO, benchmark_results)\n",
    "        calculate_complexity(paths, YOLO, profile, YAMLConfig, benchmark_results)\n",
    "        measure_inference_speed(paths, YOLO, YAMLConfig, benchmark_results)\n",
    "    generate_summary(benchmark_results)\n",
    "    print_header(\"BENCHMARK PROCESS COMPLETE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0526bf2",
   "metadata": {
    "_cell_guid": "719ccac6-2e39-4c52-ad02-ed58012bbb65",
    "_uuid": "3e53a105-7517-4a2d-986e-57bea35b8f03",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-10-31T07:26:12.100432Z",
     "iopub.status.busy": "2025-10-31T07:26:12.099970Z",
     "iopub.status.idle": "2025-10-31T07:28:51.625121Z",
     "shell.execute_reply": "2025-10-31T07:28:51.624030Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 159.548748,
     "end_time": "2025-10-31T07:28:51.626788",
     "exception": false,
     "start_time": "2025-10-31T07:26:12.078040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RT-DETR'...\r\n",
      "remote: Enumerating objects: 1100, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\r\n",
      "remote: Total 1100 (delta 8), reused 5 (delta 5), pack-reused 1077 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (1100/1100), 660.70 KiB | 11.80 MiB/s, done.\r\n",
      "Resolving deltas: 100% (522/522), done.\r\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \r\n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\r\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1761895579.272092     128 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1761895579.328940     128 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|                          Preparing Dataset for YOLO                          |\r\n",
      "================================================================================\r\n",
      "Converting instances_train2017.json: 100%|█| 4004/4004 [00:00<00:00, 29626.87it/\r\n",
      "Converting instances_val2017.json: 100%|███| 776/776 [00:00<00:00, 32238.31it/s]\r\n",
      "YOLO dataset YAML created at: /kaggle/working/taco_yolo/taco.yaml\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|                Step 1: Verifying Checkpoint and Config Files                 |\r\n",
      "================================================================================\r\n",
      "[FOUND] Distill ConvNext Finetune\r\n",
      "[FOUND] Distill ViT Finetune\r\n",
      "[FOUND] RT-DETR Baseline Finetune\r\n",
      "[FOUND] YOLOv11 Finetune\r\n",
      "\r\n",
      "---> All required files are ready!\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|               Step 2: Evaluating Performance (APval / AP50val)               |\r\n",
      "================================================================================\r\n",
      "\r\n",
      "--- Evaluating: Distill ConvNext Finetune ---\r\n",
      "\r\n",
      "---> Executing command: python tools/train.py -c /kaggle/working/temp_flattened_config.yml -r /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/best.pth --test-only\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1761895611.650754     145 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1761895611.657382     145 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Not init distributed mode.\r\n",
      "cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': 16, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/best.pth', 'tuning': None, 'epoches': 50, 'last_epoch': -1, 'use_amp': False, 'use_ema': False, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.0, 'find_unused_parameters': False, 'seed': None, 'print_freq': 100, 'checkpoint_freq': 10, 'output_dir': '/kaggle/working/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher', 'summary_dir': None, 'device': '', 'yaml_cfg': {'HybridEncoder': {'act': 'silu', 'depth_mult': 1, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'in_channels': [512, 1024, 2048], 'nhead': 8, 'num_encoder_layers': 1, 'use_encoder_idx': [2]}, 'PResNet': {'depth': 50, 'freeze_at': 0, 'freeze_norm': True, 'num_stages': 4, 'pretrained': False, 'return_idx': [1, 2, 3], 'variant': 'd'}, 'RTDETR': {'backbone': 'PResNet', 'decoder': 'RTDETRTransformerv2', 'encoder': 'HybridEncoder'}, 'RTDETRCriterionv2': {'alpha': 0.75, 'gamma': 2.0, 'losses': ['vfl', 'boxes'], 'matcher': {'alpha': 0.25, 'gamma': 2.0, 'type': 'HungarianMatcher', 'weight_dict': {'cost_bbox': 5, 'cost_class': 2, 'cost_giou': 2}}, 'weight_dict': {'loss_bbox': 5, 'loss_giou': 2, 'loss_vfl': 1}}, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'RTDETRTransformerv2': {'box_noise_scale': 1.0, 'cross_attn_method': 'default', 'eval_idx': -1, 'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'label_noise_ratio': 0.5, 'num_denoising': 100, 'num_layers': 6, 'num_levels': 3, 'num_points': [4, 4, 4], 'num_queries': 300, 'query_select_method': 'default'}, 'batch_size': 16, 'checkpoint_freq': 10, 'compile': True, 'criterion': 'RTDETRCriterionv2', 'ema': {'decay': 0.9999, 'type': 'ModelEMA', 'warmups': 2000}, 'epoches': 50, 'eval_spatial_size': [640, 640], 'evaluator': {'iou_types': ['bbox'], 'type': 'CocoEvaluator'}, 'find_unused_parameters': False, 'lr_scheduler': {'gamma': 0.1, 'milestones': [40], 'type': 'MultiStepLR'}, 'model': 'RTDETR', 'num_classes': 60, 'optimizer': {'betas': [0.9, 0.999], 'lr': 0.0001, 'params': [{'lr': 1e-05, 'params': '^(?=.*backbone)'}, {'lr': 5e-05, 'params': '^(?=.*encoder)'}], 'type': 'AdamW', 'weight_decay': 0.0001}, 'output_dir': '/kaggle/working/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher', 'postprocessor': 'RTDETRPostProcessor', 'print_freq': 100, 'remap_mscoco_category': False, 'scaler': {'enabled': True, 'type': 'GradScaler'}, 'sync_bn': True, 'task': 'detection', 'train_dataloader': {'collate_fn': {'scales': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800], 'stop_epoch': 71, 'type': 'BatchImageCollateFuncion'}, 'dataset': {'ann_file': '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_train2017.json', 'img_folder': '/kaggle/input/dsp-pre-final/processed_taco_coco/train2017', 'return_masks': False, 'transforms': {'ops': [{'p': 0.5, 'type': 'RandomPhotometricDistort'}, {'fill': 0, 'type': 'RandomZoomOut'}, {'p': 0.8, 'type': 'RandomIoUCrop'}, {'min_size': 1, 'type': 'SanitizeBoundingBoxes'}, {'type': 'RandomHorizontalFlip'}, {'size': [640, 640], 'type': 'Resize'}, {'min_size': 1, 'type': 'SanitizeBoundingBoxes'}, {'dtype': 'float32', 'scale': True, 'type': 'ConvertPILImage'}, {'fmt': 'cxcywh', 'normalize': True, 'type': 'ConvertBoxes'}], 'policy': {'epoch': 71, 'name': 'stop_epoch', 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}, 'type': 'Compose'}, 'type': 'CocoDetection'}, 'drop_last': True, 'num_workers': 4, 'shuffle': True, 'total_batch_size': 16, 'type': 'DataLoader'}, 'use_amp': False, 'use_ema': False, 'use_focal_loss': True, 'val_dataloader': {'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'dataset': {'ann_file': '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_val2017.json', 'img_folder': '/kaggle/input/dsp-pre-final/processed_taco_coco/val2017', 'return_masks': False, 'transforms': {'ops': [{'size': [640, 640], 'type': 'Resize'}, {'dtype': 'float32', 'scale': True, 'type': 'ConvertPILImage'}], 'type': 'Compose'}, 'type': 'CocoDetection'}, 'drop_last': False, 'num_workers': 4, 'shuffle': False, 'total_batch_size': 32, 'type': 'DataLoader'}, 'config': '/kaggle/working/temp_flattened_config.yml', 'resume': '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/best.pth', 'test_only': True, 'print_method': 'builtin', 'print_rank': 0}}\r\n",
      "building val_dataloader with batch_size=32...\r\n",
      "Resume checkpoint from /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_convnext_teacher/best.pth\r\n",
      "Load last_epoch\r\n",
      "Load model.state_dict\r\n",
      "Load criterion.state_dict\r\n",
      "Load postprocessor.state_dict\r\n",
      "Test:  [0/8]  eta: 0:00:33    time: 4.1340  data: 1.8593  max mem: 4717\r\n",
      "Test:  [7/8]  eta: 0:00:01    time: 1.7838  data: 0.3170  max mem: 4728\r\n",
      "Test: Total time: 0:00:14 (1.7963 s / it)\r\n",
      "Averaged stats:\r\n",
      "Accumulating evaluation results...\r\n",
      "COCOeval_opt.accumulate() finished...\r\n",
      "DONE (t=0.20s).\r\n",
      "IoU metric: bbox\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.060\r\n",
      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.082\r\n",
      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.062\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.070\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.112\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.212\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.295\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.308\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.080\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.345\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.452\r\n",
      "Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.429\r\n",
      "Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.320\r\n",
      "--- Command succeeded ---\r\n",
      "\r\n",
      "--- Evaluating: Distill ViT Finetune ---\r\n",
      "\r\n",
      "---> Executing command: python tools/train.py -c /kaggle/working/temp_flattened_config.yml -r /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth --test-only\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1761895637.786682     169 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1761895637.793111     169 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Not init distributed mode.\r\n",
      "cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': 16, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth', 'tuning': None, 'epoches': 50, 'last_epoch': -1, 'use_amp': False, 'use_ema': False, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.0, 'find_unused_parameters': False, 'seed': None, 'print_freq': 100, 'checkpoint_freq': 10, 'output_dir': '/kaggle/working/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher', 'summary_dir': None, 'device': '', 'yaml_cfg': {'HybridEncoder': {'act': 'silu', 'depth_mult': 1, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'in_channels': [512, 1024, 2048], 'nhead': 8, 'num_encoder_layers': 1, 'use_encoder_idx': [2]}, 'PResNet': {'depth': 50, 'freeze_at': 0, 'freeze_norm': True, 'num_stages': 4, 'pretrained': False, 'return_idx': [1, 2, 3], 'variant': 'd'}, 'RTDETR': {'backbone': 'PResNet', 'decoder': 'RTDETRTransformerv2', 'encoder': 'HybridEncoder'}, 'RTDETRCriterionv2': {'alpha': 0.75, 'gamma': 2.0, 'losses': ['vfl', 'boxes'], 'matcher': {'alpha': 0.25, 'gamma': 2.0, 'type': 'HungarianMatcher', 'weight_dict': {'cost_bbox': 5, 'cost_class': 2, 'cost_giou': 2}}, 'weight_dict': {'loss_bbox': 5, 'loss_giou': 2, 'loss_vfl': 1}}, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'RTDETRTransformerv2': {'box_noise_scale': 1.0, 'cross_attn_method': 'default', 'eval_idx': -1, 'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'label_noise_ratio': 0.5, 'num_denoising': 100, 'num_layers': 6, 'num_levels': 3, 'num_points': [4, 4, 4], 'num_queries': 300, 'query_select_method': 'default'}, 'batch_size': 16, 'checkpoint_freq': 10, 'compile': True, 'criterion': 'RTDETRCriterionv2', 'ema': {'decay': 0.9999, 'type': 'ModelEMA', 'warmups': 2000}, 'epoches': 50, 'eval_spatial_size': [640, 640], 'evaluator': {'iou_types': ['bbox'], 'type': 'CocoEvaluator'}, 'find_unused_parameters': False, 'lr_scheduler': {'gamma': 0.1, 'milestones': [40], 'type': 'MultiStepLR'}, 'model': 'RTDETR', 'num_classes': 60, 'optimizer': {'betas': [0.9, 0.999], 'lr': 2e-05, 'params': [{'lr': 2e-05, 'params': '^(?=.*backbone)'}], 'type': 'AdamW', 'weight_decay': 0.0001}, 'output_dir': '/kaggle/working/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher', 'postprocessor': 'RTDETRPostProcessor', 'print_freq': 100, 'remap_mscoco_category': False, 'scaler': {'enabled': True, 'type': 'GradScaler'}, 'sync_bn': True, 'task': 'detection', 'train_dataloader': {'collate_fn': {'scales': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800], 'stop_epoch': 71, 'type': 'BatchImageCollateFuncion'}, 'dataset': {'ann_file': '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_train2017.json', 'img_folder': '/kaggle/input/dsp-pre-final/processed_taco_coco/train2017', 'return_masks': False, 'transforms': {'ops': [{'p': 0.5, 'type': 'RandomPhotometricDistort'}, {'fill': 0, 'type': 'RandomZoomOut'}, {'p': 0.8, 'type': 'RandomIoUCrop'}, {'min_size': 1, 'type': 'SanitizeBoundingBoxes'}, {'type': 'RandomHorizontalFlip'}, {'size': [640, 640], 'type': 'Resize'}, {'min_size': 1, 'type': 'SanitizeBoundingBoxes'}, {'dtype': 'float32', 'scale': True, 'type': 'ConvertPILImage'}, {'fmt': 'cxcywh', 'normalize': True, 'type': 'ConvertBoxes'}], 'policy': {'epoch': 71, 'name': 'stop_epoch', 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}, 'type': 'Compose'}, 'type': 'CocoDetection'}, 'drop_last': True, 'num_workers': 4, 'shuffle': True, 'total_batch_size': 16, 'type': 'DataLoader'}, 'use_amp': False, 'use_ema': False, 'use_focal_loss': True, 'val_dataloader': {'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'dataset': {'ann_file': '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_val2017.json', 'img_folder': '/kaggle/input/dsp-pre-final/processed_taco_coco/val2017', 'return_masks': False, 'transforms': {'ops': [{'size': [640, 640], 'type': 'Resize'}, {'dtype': 'float32', 'scale': True, 'type': 'ConvertPILImage'}], 'type': 'Compose'}, 'type': 'CocoDetection'}, 'drop_last': False, 'num_workers': 4, 'shuffle': False, 'total_batch_size': 32, 'type': 'DataLoader'}, 'config': '/kaggle/working/temp_flattened_config.yml', 'resume': '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth', 'test_only': True, 'print_method': 'builtin', 'print_rank': 0}}\r\n",
      "building val_dataloader with batch_size=32...\r\n",
      "Resume checkpoint from /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_DISTILLED/rtdetrv2_finetune_taco_vit_teacher/best.pth\r\n",
      "Load last_epoch\r\n",
      "Load model.state_dict\r\n",
      "Load criterion.state_dict\r\n",
      "Load postprocessor.state_dict\r\n",
      "Test:  [0/8]  eta: 0:00:33    time: 4.2134  data: 2.1355  max mem: 4717\r\n",
      "Test:  [7/8]  eta: 0:00:01    time: 1.8144  data: 0.3424  max mem: 4728\r\n",
      "Test: Total time: 0:00:14 (1.8305 s / it)\r\n",
      "Averaged stats:\r\n",
      "Accumulating evaluation results...\r\n",
      "COCOeval_opt.accumulate() finished...\r\n",
      "DONE (t=0.20s).\r\n",
      "IoU metric: bbox\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\r\n",
      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.040\r\n",
      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.031\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.009\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.051\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.041\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.145\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.220\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.231\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.063\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.317\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.317\r\n",
      "Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.332\r\n",
      "Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.246\r\n",
      "--- Command succeeded ---\r\n",
      "\r\n",
      "--- Evaluating: RT-DETR Baseline Finetune ---\r\n",
      "\r\n",
      "---> Executing command: python tools/train.py -c /kaggle/working/temp_flattened_config.yml -r /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/best.pth --test-only\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1761895663.953650     193 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1761895663.960301     193 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Not init distributed mode.\r\n",
      "cfg:  {'task': 'detection', '_model': None, '_postprocessor': None, '_criterion': None, '_optimizer': None, '_lr_scheduler': None, '_lr_warmup_scheduler': None, '_train_dataloader': None, '_val_dataloader': None, '_ema': None, '_scaler': None, '_train_dataset': None, '_val_dataset': None, '_collate_fn': None, '_evaluator': None, '_writer': None, 'num_workers': 0, 'batch_size': 16, '_train_batch_size': None, '_val_batch_size': None, '_train_shuffle': None, '_val_shuffle': None, 'resume': '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/best.pth', 'tuning': None, 'epoches': 50, 'last_epoch': -1, 'use_amp': False, 'use_ema': False, 'ema_decay': 0.9999, 'ema_warmups': 2000, 'sync_bn': True, 'clip_max_norm': 0.0, 'find_unused_parameters': False, 'seed': None, 'print_freq': 100, 'checkpoint_freq': 10, 'output_dir': '/kaggle/working/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE', 'summary_dir': None, 'device': '', 'yaml_cfg': {'HybridEncoder': {'act': 'silu', 'depth_mult': 1, 'dim_feedforward': 1024, 'dropout': 0.0, 'enc_act': 'gelu', 'expansion': 1.0, 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'in_channels': [512, 1024, 2048], 'nhead': 8, 'num_encoder_layers': 1, 'use_encoder_idx': [2]}, 'PResNet': {'depth': 50, 'freeze_at': 0, 'freeze_norm': True, 'num_stages': 4, 'pretrained': False, 'return_idx': [1, 2, 3], 'variant': 'd'}, 'RTDETR': {'backbone': 'PResNet', 'decoder': 'RTDETRTransformerv2', 'encoder': 'HybridEncoder'}, 'RTDETRCriterionv2': {'alpha': 0.75, 'gamma': 2.0, 'losses': ['vfl', 'boxes'], 'matcher': {'alpha': 0.25, 'gamma': 2.0, 'type': 'HungarianMatcher', 'weight_dict': {'cost_bbox': 5, 'cost_class': 2, 'cost_giou': 2}}, 'weight_dict': {'loss_bbox': 5, 'loss_giou': 2, 'loss_vfl': 1}}, 'RTDETRPostProcessor': {'num_top_queries': 300}, 'RTDETRTransformerv2': {'box_noise_scale': 1.0, 'cross_attn_method': 'default', 'eval_idx': -1, 'feat_channels': [256, 256, 256], 'feat_strides': [8, 16, 32], 'hidden_dim': 256, 'label_noise_ratio': 0.5, 'num_denoising': 100, 'num_layers': 6, 'num_levels': 3, 'num_points': [4, 4, 4], 'num_queries': 300, 'query_select_method': 'default'}, 'batch_size': 16, 'checkpoint_freq': 10, 'compile': True, 'criterion': 'RTDETRCriterionv2', 'ema': {'decay': 0.9999, 'type': 'ModelEMA', 'warmups': 2000}, 'epoches': 50, 'eval_spatial_size': [640, 640], 'evaluator': {'iou_types': ['bbox'], 'type': 'CocoEvaluator'}, 'find_unused_parameters': False, 'lr_scheduler': {'gamma': 0.1, 'milestones': [40], 'type': 'MultiStepLR'}, 'model': 'RTDETR', 'num_classes': 60, 'optimizer': {'betas': [0.9, 0.999], 'lr': 2e-05, 'params': [{'lr': 2e-05, 'params': '^(?=.*backbone)'}], 'type': 'AdamW', 'weight_decay': 0.0001}, 'output_dir': '/kaggle/working/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE', 'postprocessor': 'RTDETRPostProcessor', 'print_freq': 100, 'remap_mscoco_category': False, 'scaler': {'enabled': True, 'type': 'GradScaler'}, 'sync_bn': True, 'task': 'detection', 'train_dataloader': {'collate_fn': {'scales': [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800], 'stop_epoch': 71, 'type': 'BatchImageCollateFuncion'}, 'dataset': {'ann_file': '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_train2017.json', 'img_folder': '/kaggle/input/dsp-pre-final/processed_taco_coco/train2017', 'return_masks': False, 'transforms': {'ops': [{'p': 0.5, 'type': 'RandomPhotometricDistort'}, {'fill': 0, 'type': 'RandomZoomOut'}, {'p': 0.8, 'type': 'RandomIoUCrop'}, {'min_size': 1, 'type': 'SanitizeBoundingBoxes'}, {'type': 'RandomHorizontalFlip'}, {'size': [640, 640], 'type': 'Resize'}, {'min_size': 1, 'type': 'SanitizeBoundingBoxes'}, {'dtype': 'float32', 'scale': True, 'type': 'ConvertPILImage'}, {'fmt': 'cxcywh', 'normalize': True, 'type': 'ConvertBoxes'}], 'policy': {'epoch': 71, 'name': 'stop_epoch', 'ops': ['RandomPhotometricDistort', 'RandomZoomOut', 'RandomIoUCrop']}, 'type': 'Compose'}, 'type': 'CocoDetection'}, 'drop_last': True, 'num_workers': 4, 'shuffle': True, 'total_batch_size': 16, 'type': 'DataLoader'}, 'use_amp': False, 'use_ema': False, 'use_focal_loss': True, 'val_dataloader': {'collate_fn': {'type': 'BatchImageCollateFuncion'}, 'dataset': {'ann_file': '/kaggle/input/dsp-pre-final/processed_taco_coco/annotations/instances_val2017.json', 'img_folder': '/kaggle/input/dsp-pre-final/processed_taco_coco/val2017', 'return_masks': False, 'transforms': {'ops': [{'size': [640, 640], 'type': 'Resize'}, {'dtype': 'float32', 'scale': True, 'type': 'ConvertPILImage'}], 'type': 'Compose'}, 'type': 'CocoDetection'}, 'drop_last': False, 'num_workers': 4, 'shuffle': False, 'total_batch_size': 32, 'type': 'DataLoader'}, 'config': '/kaggle/working/temp_flattened_config.yml', 'resume': '/kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/best.pth', 'test_only': True, 'print_method': 'builtin', 'print_rank': 0}}\r\n",
      "building val_dataloader with batch_size=32...\r\n",
      "Resume checkpoint from /kaggle/input/rt-detr-dinov3-distilled-model/FINAL/FINETUNE_BASELINE/rtdetrv2_finetune_taco_finetune_BASELINE/best.pth\r\n",
      "Load last_epoch\r\n",
      "Load model.state_dict\r\n",
      "Load criterion.state_dict\r\n",
      "Load postprocessor.state_dict\r\n",
      "Test:  [0/8]  eta: 0:00:31    time: 3.9260  data: 1.8353  max mem: 4717\r\n",
      "Test:  [7/8]  eta: 0:00:01    time: 1.8102  data: 0.3153  max mem: 4728\r\n",
      "Test: Total time: 0:00:14 (1.8298 s / it)\r\n",
      "Averaged stats:\r\n",
      "Accumulating evaluation results...\r\n",
      "COCOeval_opt.accumulate() finished...\r\n",
      "DONE (t=0.20s).\r\n",
      "IoU metric: bbox\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.031\r\n",
      "Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.040\r\n",
      "Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.032\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.063\r\n",
      "Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.149\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.217\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.230\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.062\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.302\r\n",
      "Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.337\r\n",
      "Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.337\r\n",
      "Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.228\r\n",
      "--- Command succeeded ---\r\n",
      "\r\n",
      "--- Evaluating: YOLOv11 Finetune ---\r\n",
      "Ultralytics 8.3.223 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\r\n",
      "YOLO11l summary (fused): 190 layers, 25,325,572 parameters, 0 gradients, 86.8 GFLOPs\r\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 16.8MB/s 0.0s\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2858.3±882.9 MB/s, size: 277.6 KB)\r\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/taco_yolo/labels/val... 225 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 225/225 1.4Kit/s 0.2s\r\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/taco_yolo/labels/val.cache\r\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 15/15 1.6it/s 9.2s\r\n",
      "                   all        225        776      0.403      0.265      0.264      0.229\r\n",
      "        Aluminium foil          6          7          0          0     0.0521     0.0387\r\n",
      "  Other plastic bottle          7          8      0.205       0.25      0.221      0.214\r\n",
      "  Clear plastic bottle         36         46      0.577       0.63       0.64      0.527\r\n",
      "          Glass bottle          5          6      0.205      0.167      0.129      0.114\r\n",
      "    Plastic bottle cap         36         44      0.547      0.409      0.477      0.315\r\n",
      "      Metal bottle cap         10         10      0.527        0.6      0.627       0.56\r\n",
      "          Broken glass          3          8          0          0          0          0\r\n",
      "              Food Can          3          8      0.112      0.125      0.207      0.179\r\n",
      "               Aerosol          3          3      0.307      0.333      0.349      0.349\r\n",
      "             Drink can         24         46      0.605      0.633      0.613      0.452\r\n",
      "          Other carton          9          9     0.0462      0.111      0.162       0.15\r\n",
      "            Egg carton          2          3          1          0          0          0\r\n",
      "          Drink carton          5          5      0.211        0.2       0.24       0.22\r\n",
      "     Corrugated carton          5          7      0.483      0.714      0.582      0.551\r\n",
      "           Meal carton          3          4      0.681        0.5      0.559      0.482\r\n",
      "             Paper cup         11         11      0.137     0.0909      0.183      0.105\r\n",
      "Disposable plastic cup          8         10      0.485        0.6      0.434      0.337\r\n",
      "              Foam cup          3          4          0          0          0          0\r\n",
      "             Glass cup          1          1          1          0          0          0\r\n",
      "            Food waste          2          2          0          0          0          0\r\n",
      "             Glass jar          1          2      0.883        0.5      0.578      0.578\r\n",
      "           Plastic lid         14         18       0.54      0.278      0.271      0.225\r\n",
      "             Metal lid          2          3          1          0          0          0\r\n",
      "         Other plastic         32         44      0.215      0.181       0.15      0.117\r\n",
      "               Tissues          5          6      0.208      0.167     0.0625     0.0545\r\n",
      "        Wrapping paper          5          5          0          0     0.0259     0.0186\r\n",
      "          Normal paper         11         15      0.125      0.133      0.131      0.117\r\n",
      "             Paper bag          3          3      0.631      0.667      0.448      0.448\r\n",
      "          Plastic film         40         60      0.304      0.317      0.268      0.221\r\n",
      "        Six pack rings          1          1      0.609          1      0.995      0.995\r\n",
      "           Garbage bag          3          4      0.252       0.25      0.268      0.243\r\n",
      " Other plastic wrapper         26         61      0.219      0.286       0.24       0.16\r\n",
      "Single-use carrier bag          7          9      0.111      0.222      0.152      0.118\r\n",
      "          Crisp packet          7          8      0.171       0.25      0.254      0.204\r\n",
      "            Spread tub          2          2          1          0          0          0\r\n",
      "Disposable food container          6          6      0.357      0.333      0.345      0.345\r\n",
      "   Foam food container          2          3      0.632          1      0.995      0.963\r\n",
      "       Plastic glooves          2          2      0.686        0.5      0.523      0.523\r\n",
      "      Plastic utensils          3          3          1          0     0.0601     0.0601\r\n",
      "               Pop tab         15         24      0.442      0.101      0.117     0.0889\r\n",
      "        Rope & strings          4          4      0.234       0.25      0.249      0.149\r\n",
      "                  Shoe          1          2          0          0          0          0\r\n",
      "       Squeezable tube          2          2          0          0          0          0\r\n",
      "         Plastic straw         18         22      0.582      0.318      0.369      0.284\r\n",
      "           Paper straw          2          2          1          0          0          0\r\n",
      "       Styrofoam piece         15         20      0.522        0.4      0.464      0.398\r\n",
      "      Unlabeled litter         44        124      0.205     0.0968     0.0734     0.0326\r\n",
      "             Cigarette         29         79      0.301      0.127      0.145     0.0736\r\n",
      "Speed: 2.1ms preprocess, 31.0ms inference, 0.0ms loss, 0.7ms postprocess per image\r\n",
      "Results saved to \u001b[1m/kaggle/working/runs/detect/val\u001b[0m\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|             Step 3: Analyzing Model Complexity (Params & FLOPs)              |\r\n",
      "================================================================================\r\n",
      "\r\n",
      "--- Analyzing: Distill ConvNext Finetune ---\r\n",
      "Params: 40.92 M, FLOPs: 136.06 G\r\n",
      "\r\n",
      "--- Analyzing: Distill ViT Finetune ---\r\n",
      "Params: 40.92 M, FLOPs: 136.06 G\r\n",
      "\r\n",
      "--- Analyzing: RT-DETR Baseline Finetune ---\r\n",
      "Params: 40.92 M, FLOPs: 136.06 G\r\n",
      "\r\n",
      "--- Analyzing: YOLOv11 Finetune ---\r\n",
      "Params: 25.36 M, FLOPs: 87.53 G\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|                      Step 4: Measuring Inference Speed                       |\r\n",
      "================================================================================\r\n",
      "\r\n",
      "--- Measuring: Distill ConvNext Finetune ---\r\n",
      "Average Inference Time: 58.76 ms/image\r\n",
      "\r\n",
      "--- Measuring: Distill ViT Finetune ---\r\n",
      "Average Inference Time: 59.04 ms/image\r\n",
      "\r\n",
      "--- Measuring: RT-DETR Baseline Finetune ---\r\n",
      "Average Inference Time: 59.94 ms/image\r\n",
      "\r\n",
      "--- Measuring: YOLOv11 Finetune ---\r\n",
      "Average Inference Time: 29.42 ms/image\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|                           Final Benchmark Summary                            |\r\n",
      "================================================================================\r\n",
      "Benchmark conducted on: Tesla T4\r\n",
      "                          mAP50-95   mAP50 Speed (ms) Params (M) FLOPs (G)\r\n",
      "Distill ConvNext Finetune   0.0600  0.0820      58.76      40.92    136.06\r\n",
      "Distill ViT Finetune        0.0290  0.0400      59.04      40.92    136.06\r\n",
      "RT-DETR Baseline Finetune   0.0310  0.0400      59.94      40.92    136.06\r\n",
      "YOLOv11 Finetune            0.2294  0.2637      29.42      25.36     87.53\r\n",
      "\r\n",
      "Summary table saved to benchmark_summary.csv\r\n",
      "Figure(1000x600)\r\n",
      "Summary plot saved to benchmark_plot.png\r\n",
      "\r\n",
      "================================================================================\r\n",
      "|                          BENCHMARK PROCESS COMPLETE                          |\r\n",
      "================================================================================\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf /kaggle/working/RT-DETR\n",
    "!rm -rf /kaggle/working/taco_yolo\n",
    "!git clone https://github.com/lyuwenyu/RT-DETR.git\n",
    "!python /kaggle/working/final_benchmark.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8469591,
     "sourceId": 13441245,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8616212,
     "sourceId": 13564373,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31155,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 280.265847,
   "end_time": "2025-10-31T07:28:51.970055",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-31T07:24:11.704208",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
